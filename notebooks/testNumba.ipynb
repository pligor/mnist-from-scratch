{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Practical: Coursework 2\n",
    "## Experiment 1/3\n",
    "\n",
    "**Due date: 16:00 Thursday 24th November 2016**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Georgios Pligoropoulos - s1687568"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numba import autojit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def checkVarIsDefined(varName):\n",
    "    try:\n",
    "        exec(varName)\n",
    "    except NameError:\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "mlpdir = '/home/student/Dropbox/msc_Artificial_Intelligence/mlp_Machine_Learning_Practical/mlpractical'\n",
    "sys.path.append(mlpdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "#%matplotlib notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from mlp.data_providers import MNISTDataProvider\n",
    "\n",
    "from mlp.layers import AffineLayer, SoftmaxLayer, SigmoidLayer\n",
    "\n",
    "#from mlp.errors import CrossEntropySoftmaxError\n",
    "\n",
    "#from mlp.models import MultipleLayerModel\n",
    "\n",
    "from mlp.initialisers import ConstantInit, GlorotUniformInit\n",
    "\n",
    "#from mlp.optimisers import Optimiser\n",
    "\n",
    "from mlp.learning_rules import GradientDescentLearningRule, MomentumLearningRule, \\\n",
    "    AdaGradLearningRule, RmsPropLearningRule\n",
    "    \n",
    "from mlp.schedulers import MomentumCoefficientScheduler, ConstantLearningRateScheduler,\\\n",
    "    ReciprocalLearningRateScheduler, ExponentialLearningRateScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CrossEntropySoftmaxError(object):\n",
    "    \"\"\"Multi-class cross entropy error with Softmax applied to outputs.\"\"\"\n",
    "\n",
    "    def __call__(self, outputs, targets):\n",
    "        \"\"\"Calculates error function given a batch of outputs and targets.\n",
    "\n",
    "        Args:\n",
    "            outputs: Array of model outputs of shape (batch_size, output_dim).\n",
    "            targets: Array of target outputs of shape (batch_size, output_dim).\n",
    "\n",
    "        Returns:\n",
    "            Scalar error function value.\n",
    "        \"\"\"\n",
    "        # subtract max inside exponential to improve numerical stability -\n",
    "        # when we divide through by sum this term cancels\n",
    "        probs = np.exp(outputs - outputs.max(-1)[:, None])\n",
    "        probs /= probs.sum(-1)[:, None]\n",
    "        return -np.mean(np.sum(targets * np.log(probs), axis=1))\n",
    "\n",
    "    @autojit\n",
    "    def grad(self, outputs, targets):\n",
    "        \"\"\"Calculates gradient of error function with respect to outputs.\n",
    "\n",
    "        Args:\n",
    "            outputs: Array of model outputs of shape (batch_size, output_dim).\n",
    "            targets: Array of target outputs of shape (batch_size, output_dim).\n",
    "\n",
    "        Returns:\n",
    "            Gradient of error function with respect to outputs.\n",
    "        \"\"\"\n",
    "        # subtract max inside exponential to improve numerical stability -\n",
    "        # when we divide through by sum this term cancels\n",
    "        probs = np.exp(outputs - outputs.max(-1)[:, None])\n",
    "        probs /= probs.sum(-1)[:, None]\n",
    "        return (probs - targets) / outputs.shape[0]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return 'CrossEntropySoftmaxError'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from mlp.layers import LayerWithParameters, StochasticLayer\n",
    "\n",
    "class MultipleLayerModel(object):\n",
    "    \"\"\"A model consisting of multiple layers applied sequentially.\"\"\"\n",
    "\n",
    "    def __init__(self, layers):\n",
    "        \"\"\"Create a new multiple layer model instance.\n",
    "\n",
    "        Args:\n",
    "            layers: List of the the layer objecst defining the model in the\n",
    "                order they should be applied from inputs to outputs.\n",
    "        \"\"\"\n",
    "        self.layers = layers\n",
    "\n",
    "    @property\n",
    "    def params(self):\n",
    "        \"\"\"A list of all of the parameters of the model.\"\"\"\n",
    "        params = []\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, LayerWithParameters):\n",
    "                params += layer.params\n",
    "        return params\n",
    "\n",
    "    @autojit\n",
    "    def fprop(self, inputs, stochastic=True):\n",
    "        \"\"\"Forward propagates a batch of inputs through the model.\n",
    "\n",
    "        Args:\n",
    "            inputs: Batch of inputs to the model.\n",
    "            stochastic: Whether to use stochastic forward propagation\n",
    "                for stochastic layers (True) or deterministic (False).\n",
    "\n",
    "        Returns:\n",
    "            List of the activations at the output of all layers of the model\n",
    "            plus the inputs (to the first layer) as the first element. The\n",
    "            last element of the list corresponds to the model outputs.\n",
    "        \"\"\"\n",
    "        activations = [inputs]\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            if isinstance(layer, StochasticLayer):\n",
    "                activations.append(self.layers[i].fprop(\n",
    "                    activations[i], stochastic=stochastic))\n",
    "            else:\n",
    "                activations.append(self.layers[i].fprop(activations[i]))\n",
    "        return activations\n",
    "\n",
    "    #@autojit\n",
    "    #@jit(nopython=True)\n",
    "    @autojit(nopython=True)\n",
    "    def grads_wrt_params(self, activations, grads_wrt_outputs):\n",
    "        \"\"\"Calculates gradients with respect to the model parameters.\n",
    "\n",
    "        Args:\n",
    "            activations: List of all activations from forward pass through\n",
    "                model using `fprop`.\n",
    "            grads_wrt_outputs: Gradient with respect to the model outputs of\n",
    "               the scalar function parameter gradients are being calculated\n",
    "               for.\n",
    "\n",
    "        Returns:\n",
    "            List of gradients of the scalar function with respect to all model\n",
    "            parameters.\n",
    "        \"\"\"\n",
    "        grads_wrt_params = []\n",
    "        for i, layer in enumerate(self.layers[::-1]):\n",
    "            inputs = activations[-i - 2]\n",
    "            outputs = activations[-i - 1]\n",
    "            if isinstance(layer, LayerWithParameters):\n",
    "                # Gradients are appended in reversed order, going backwards\n",
    "                # through layers, so grads_wrt_params can be reversed at end to\n",
    "                # give gradients in consistent order with self.params\n",
    "                grads_wrt_params += layer.grads_wrt_params(inputs, grads_wrt_outputs)[::-1]\n",
    "            # If not at first layer back-propagate gradients\n",
    "            if i != len(self.layers) - 1:\n",
    "                grads_wrt_outputs = layer.bprop(\n",
    "                    inputs, outputs, grads_wrt_outputs)\n",
    "        return grads_wrt_params[::-1]\n",
    "\n",
    "    def params_penalty(self):\n",
    "        \"\"\"Calculates the parameter dependent penalty term of the model.\"\"\"\n",
    "        params_penalty = 0.\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, LayerWithParameters):\n",
    "                params_penalty += layer.params_penalty()\n",
    "        return params_penalty\n",
    "\n",
    "    def __repr__(self):\n",
    "        return (\n",
    "            'MultiLayerModel(\\n    ' +\n",
    "            '\\n    '.join([str(layer) for layer in self.layers]) +\n",
    "            '\\n)'\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import logging\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class Optimiser(object):\n",
    "    \"\"\"Basic model optimiser.\"\"\"\n",
    "\n",
    "    def __init__(self, model, error, learning_rule, train_dataset,\n",
    "                 valid_dataset=None, data_monitors=None, schedulers=[],\n",
    "                 use_stochastic_eval=True):\n",
    "        \"\"\"Create a new optimiser instance.\n",
    "\n",
    "        Args:\n",
    "            model: The model to optimise.\n",
    "            error: The scalar error function to minimise.\n",
    "            learning_rule: Gradient based learning rule to use to minimise\n",
    "                error.\n",
    "            train_dataset: Data provider for training set data batches.\n",
    "            valid_dataset: Data provider for validation set data batches.\n",
    "            data_monitors: Dictionary of functions evaluated on targets and\n",
    "                model outputs (averaged across both full training and\n",
    "                validation data sets) to monitor during training in addition\n",
    "                to the error. Keys should correspond to a string label for\n",
    "                the statistic being evaluated.\n",
    "            schedulers: List of learning rule scheduler objects for adjusting\n",
    "                learning rule hyperparameters over training. Can be empty.\n",
    "            use_stochastic_eval: Whether to use `stochastic=True` flag in\n",
    "                `model.fprop` for evaluating model performance during training.\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.error = error\n",
    "        self.learning_rule = learning_rule\n",
    "        self.learning_rule.initialise(self.model.params)\n",
    "        self.train_dataset = train_dataset\n",
    "        self.valid_dataset = valid_dataset\n",
    "        self.data_monitors = OrderedDict([('error', error)])\n",
    "        if data_monitors is not None:\n",
    "            self.data_monitors.update(data_monitors)\n",
    "        self.schedulers = schedulers\n",
    "        self.use_stochastic_eval = use_stochastic_eval\n",
    "\n",
    "    #@autojit\n",
    "    def do_training_epoch(self):\n",
    "        \"\"\"Do a single training epoch.\n",
    "\n",
    "        This iterates through all batches in training dataset, for each\n",
    "        calculating the gradient of the estimated error given the batch with\n",
    "        respect to all the model parameters and then updates the model\n",
    "        parameters according to the learning rule.\n",
    "        \"\"\"\n",
    "        for inputs_batch, targets_batch in self.train_dataset:\n",
    "            activations = self.model.fprop(inputs_batch)\n",
    "            grads_wrt_outputs = self.error.grad(activations[-1], targets_batch)\n",
    "            grads_wrt_params = self.model.grads_wrt_params(\n",
    "                activations, grads_wrt_outputs)\n",
    "            self.learning_rule.update_params(grads_wrt_params)\n",
    "\n",
    "    def eval_monitors(self, dataset, label):\n",
    "        \"\"\"Evaluates the monitors for the given dataset.\n",
    "\n",
    "        Args:\n",
    "            dataset: Dataset to perform evaluation with.\n",
    "            label: Tag to add to end of monitor keys to identify dataset.\n",
    "\n",
    "        Returns:\n",
    "            OrderedDict of monitor values evaluated on dataset.\n",
    "        \"\"\"\n",
    "        data_mon_vals = OrderedDict([(key + label, 0.) for key\n",
    "                                     in self.data_monitors.keys()])\n",
    "        for inputs_batch, targets_batch in dataset:\n",
    "            activations = self.model.fprop(\n",
    "                inputs_batch, stochastic=self.use_stochastic_eval)\n",
    "            for key, data_monitor in self.data_monitors.items():\n",
    "                data_mon_vals[key + label] += data_monitor(\n",
    "                    activations[-1], targets_batch)\n",
    "        for key, data_monitor in self.data_monitors.items():\n",
    "            data_mon_vals[key + label] /= dataset.num_batches\n",
    "        return data_mon_vals\n",
    "\n",
    "    def get_epoch_stats(self):\n",
    "        \"\"\"Computes training statistics for an epoch.\n",
    "\n",
    "        Returns:\n",
    "            An OrderedDict with keys corresponding to the statistic labels and\n",
    "            values corresponding to the value of the statistic.\n",
    "        \"\"\"\n",
    "        epoch_stats = OrderedDict()\n",
    "        epoch_stats.update(self.eval_monitors(self.train_dataset, '(train)'))\n",
    "        if self.valid_dataset is not None:\n",
    "            epoch_stats.update(self.eval_monitors(\n",
    "                self.valid_dataset, '(valid)'))\n",
    "        epoch_stats['params_penalty'] = self.model.params_penalty()\n",
    "        return epoch_stats\n",
    "\n",
    "    def log_stats(self, epoch, epoch_time, stats):\n",
    "        \"\"\"Outputs stats for a training epoch to a logger.\n",
    "\n",
    "        Args:\n",
    "            epoch (int): Epoch counter.\n",
    "            epoch_time: Time taken in seconds for the epoch to complete.\n",
    "            stats: Monitored stats for the epoch.\n",
    "        \"\"\"\n",
    "        logger.info('Epoch {0}: {1:.2f}s to complete\\n  {2}'.format(\n",
    "            epoch, epoch_time,\n",
    "            ', '.join(['{0}={1:.2e}'.format(k, v) for (k, v) in stats.items()])\n",
    "        ))\n",
    "\n",
    "    #@autojit\n",
    "    def train(self, num_epochs, stats_interval=5):\n",
    "        \"\"\"Trains a model for a set number of epochs.\n",
    "\n",
    "        Args:\n",
    "            num_epochs: Number of epochs (complete passes through trainin\n",
    "                dataset) to train for.\n",
    "            stats_interval: Training statistics will be recorded and logged\n",
    "                every `stats_interval` epochs.\n",
    "\n",
    "        Returns:\n",
    "            Tuple with first value being an array of training run statistics,\n",
    "            the second being a dict mapping the labels for the statistics\n",
    "            recorded to their column index in the array and the final value\n",
    "            being the total time elapsed in seconds during the training run.\n",
    "        \"\"\"\n",
    "        stats = self.get_epoch_stats()\n",
    "        logger.info(\n",
    "            'Epoch 0:\\n  ' +\n",
    "            ', '.join(['{0}={1:.2e}'.format(k, v) for (k, v) in stats.items()])\n",
    "        )\n",
    "        run_stats = [stats.values()]\n",
    "        run_start_time = time.time()\n",
    "        for epoch in range(1, num_epochs + 1):\n",
    "            for scheduler in self.schedulers:\n",
    "                scheduler.update_learning_rule(self.learning_rule, epoch - 1)\n",
    "            start_time = time.time()\n",
    "            self.do_training_epoch()\n",
    "            epoch_time = time.time() - start_time\n",
    "            if epoch % stats_interval == 0:\n",
    "                stats = self.get_epoch_stats()\n",
    "                self.log_stats(epoch, epoch_time, stats)\n",
    "                run_stats.append(stats.values())\n",
    "        run_time = time.time() - run_start_time\n",
    "        return (\n",
    "            np.array(run_stats),\n",
    "            {k: i for i, k in enumerate(stats.keys())},\n",
    "            run_time\n",
    "        )\n",
    "        return np.array(run_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(model, error, learning_rule, train_data, valid_data, num_epochs, stats_interval, schedulers=[]):\n",
    "    # As well as monitoring the error over training also monitor classification\n",
    "    # accuracy i.e. proportion of most-probable predicted classes being equal to targets\n",
    "    data_monitors={'acc': lambda y, t: (y.argmax(-1) == t.argmax(-1)).mean()}\n",
    "\n",
    "    # Use the created objects to initialise a new Optimiser instance.\n",
    "    optimiser = Optimiser(\n",
    "        model, error, learning_rule, train_data, valid_data, data_monitors, schedulers = schedulers)\n",
    "\n",
    "    # Run the optimiser for some number of epochs (full passes through the training set)\n",
    "    # printing statistics every epoch\n",
    "    stats, keys, runTime = optimiser.train(num_epochs=num_epochs, stats_interval=stats_interval)\n",
    "    #stats = optimiser.train(num_epochs=num_epochs, stats_interval=stats_interval)\n",
    "    \n",
    "#     keys = {'acc(train)': 1,\n",
    "#      'acc(valid)': 3,\n",
    "#      'error(train)': 0,\n",
    "#      'error(valid)': 2,\n",
    "#      'params_penalty': 4}\n",
    "    \n",
    "#     runTime = 0\n",
    "    \n",
    "    return stats, keys, runTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model_and_plot_stats(\n",
    "        model, error, learning_rule, train_data, valid_data, num_epochs, stats_interval, schedulers=[]):\n",
    "    \n",
    "    stats, keys, runTime = train_model(model, error, learning_rule, train_data, valid_data,\n",
    "                                       num_epochs, stats_interval, schedulers)\n",
    "\n",
    "    # Plot the change in the validation and training set error over training.\n",
    "    fig_1 = plt.figure(figsize=(8, 4))\n",
    "    ax_1 = fig_1.add_subplot(111)\n",
    "    for k in ['error(train)', 'error(valid)']:\n",
    "        ax_1.plot(np.arange(1, stats.shape[0]) * stats_interval, \n",
    "                  stats[1:, keys[k]], label=k)\n",
    "    ax_1.legend(loc=0)\n",
    "    ax_1.set_xlabel('Epoch number')\n",
    "\n",
    "    # Plot the change in the validation and training set accuracy over training.\n",
    "    fig_2 = plt.figure(figsize=(8, 4))\n",
    "    ax_2 = fig_2.add_subplot(111)\n",
    "    for k in ['acc(train)', 'acc(valid)']:\n",
    "        ax_2.plot(np.arange(1, stats.shape[0]) * stats_interval, \n",
    "                  stats[1:, keys[k]], label=k)\n",
    "    ax_2.legend(loc=0)\n",
    "    ax_2.set_xlabel('Epoch number')\n",
    "    \n",
    "    return stats, keys, fig_1, ax_1, fig_2, ax_2, runTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getFinalValues(statistics):\n",
    "    lastStats = statistics[-1]\n",
    "    return {\n",
    "        \"Final Training Error\": lastStats[0],\n",
    "        \"Final Training Accuracy\": lastStats[1],\n",
    "        \"Final Testing Error\": lastStats[2],\n",
    "        \"Final Testing Accuracy\": lastStats[3],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "seed = 16011984\n",
    "rng = np.random.RandomState(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: When in our explanations refer to **accuracy** we mean the final validation accuracy and when we refer to performance we mean how fast we reached the optimal accuracy for the current experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grand Experiment 1: Exploring the system architecture in numbers of layers and numbers of nodes\n",
    "\n",
    "We will explore where the systems breaks/fails for too few layers or too few features and where for too many of them\n",
    "\n",
    "We are going to a simple system with constant learning rate since here we are not optimizing for accuracy neither optimizing for performance. Rather we want to see how the complexity of the model generally affects the  accuracy and performance on a dataset as complex as MNIST. In other words we care for comparison rather than absolute values.\n",
    "\n",
    "We will use the constant learning rate of 1.1 because it performed relatively well, with good accuracy and performance without making the system too unstable\n",
    "\n",
    "Note: we are using a high learning rate that produced good enough results within 20 epochs in the previous experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights_init = GlorotUniformInit(rng=rng)\n",
    "biases_init = ConstantInit(0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Seed a random number generator\n",
    "\n",
    "# Set up a logger object to print info about the training run to stdout\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.handlers = [logging.StreamHandler()]\n",
    "\n",
    "# Create data provider objects for the MNIST data set\n",
    "train_data = MNISTDataProvider('train', batch_size=50, rng=rng)\n",
    "valid_data = MNISTDataProvider('valid', batch_size=50, rng=rng)\n",
    "\n",
    "error = CrossEntropySoftmaxError() #this does not contain any immutable data that's why it can be set only once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stats_interval = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constant Learning Rate 1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Three Layers and dimensionality of 100 for each hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0:\n",
      "  error(train)=2.77e+00, acc(train)=9.94e-02, error(valid)=2.77e+00, acc(valid)=9.90e-02, params_penalty=0.00e+00\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Caused By:\nTraceback (most recent call last):\n  File \"/home/student/anaconda2/envs/mlp/lib/python2.7/site-packages/numba/compiler.py\", line 249, in run\n    stage()\n  File \"/home/student/anaconda2/envs/mlp/lib/python2.7/site-packages/numba/compiler.py\", line 459, in stage_nopython_frontend\n    legalize_given_types(self.args, self.return_type)\n  File \"/home/student/anaconda2/envs/mlp/lib/python2.7/site-packages/numba/compiler.py\", line 727, in legalize_given_types\n    \"mode\" % (i, a))\nTypeError: Arg 0 of pyobject is not legal in nopython mode\n\nFailed at nopython (nopython frontend)\nArg 0 of pyobject is not legal in nopython mode",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-119b9b4a273c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mlearning_rule\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstats_interval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mschedulers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mConstantLearningRateScheduler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m )\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-fc7da9e9903b>\u001b[0m in \u001b[0;36mtrain_model_and_plot_stats\u001b[0;34m(model, error, learning_rule, train_data, valid_data, num_epochs, stats_interval, schedulers)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     stats, keys, runTime = train_model(model, error, learning_rule, train_data, valid_data,\n\u001b[0;32m----> 5\u001b[0;31m                                        num_epochs, stats_interval, schedulers)\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# Plot the change in the validation and training set error over training.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-856eeb768660>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, error, learning_rule, train_data, valid_data, num_epochs, stats_interval, schedulers)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# Run the optimiser for some number of epochs (full passes through the training set)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# printing statistics every epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mstats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunTime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimiser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstats_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstats_interval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0;31m#stats = optimiser.train(num_epochs=num_epochs, stats_interval=stats_interval)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-763fdc23e34e>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, num_epochs, stats_interval)\u001b[0m\n\u001b[1;32m    138\u001b[0m                 \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_learning_rule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_rule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_training_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m             \u001b[0mepoch_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mstats_interval\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-763fdc23e34e>\u001b[0m in \u001b[0;36mdo_training_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0mgrads_wrt_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivations\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             grads_wrt_params = self.model.grads_wrt_params(\n\u001b[0;32m---> 60\u001b[0;31m                 activations, grads_wrt_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_rule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads_wrt_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/student/anaconda2/envs/mlp/lib/python2.7/site-packages/numba/dispatcher.pyc\u001b[0m in \u001b[0;36m_compile_for_args\u001b[0;34m(self, *args, **kws)\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m                 \u001b[0mreal_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtypeof_pyval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minspect_llvm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/student/anaconda2/envs/mlp/lib/python2.7/site-packages/numba/dispatcher.pyc\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(self, sig)\u001b[0m\n\u001b[1;32m    493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cache_misses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 495\u001b[0;31m             \u001b[0mcres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    496\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_overload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_overload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/student/anaconda2/envs/mlp/lib/python2.7/site-packages/numba/dispatcher.pyc\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(self, args, return_type)\u001b[0m\n\u001b[1;32m     74\u001b[0m                                       \u001b[0mimpl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                                       \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m                                       flags=flags, locals=self.locals)\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0;31m# Check typing error if object mode is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtyping_error\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_pyobject\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/student/anaconda2/envs/mlp/lib/python2.7/site-packages/numba/compiler.pyc\u001b[0m in \u001b[0;36mcompile_extra\u001b[0;34m(typingctx, targetctx, func, args, return_type, flags, locals, library)\u001b[0m\n\u001b[1;32m    694\u001b[0m     pipeline = Pipeline(typingctx, targetctx, library,\n\u001b[1;32m    695\u001b[0m                         args, return_type, flags, locals)\n\u001b[0;32m--> 696\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile_extra\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    697\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/student/anaconda2/envs/mlp/lib/python2.7/site-packages/numba/compiler.pyc\u001b[0m in \u001b[0;36mcompile_extra\u001b[0;34m(self, func)\u001b[0m\n\u001b[1;32m    367\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 369\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile_bytecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc_attr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc_attr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m     def compile_bytecode(self, bc, lifted=(), lifted_from=None,\n",
      "\u001b[0;32m/home/student/anaconda2/envs/mlp/lib/python2.7/site-packages/numba/compiler.pyc\u001b[0m in \u001b[0;36mcompile_bytecode\u001b[0;34m(self, bc, lifted, lifted_from, func_attr)\u001b[0m\n\u001b[1;32m    376\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlifted_from\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlifted_from\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc_attr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_attr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compile_bytecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcompile_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc_attr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDEFAULT_FUNCTION_ATTRIBUTES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/student/anaconda2/envs/mlp/lib/python2.7/site-packages/numba/compiler.pyc\u001b[0m in \u001b[0;36m_compile_bytecode\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m         \u001b[0mpm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 662\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    663\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    664\u001b[0m             \u001b[0;31m# Early pipeline completion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/student/anaconda2/envs/mlp/lib/python2.7/site-packages/numba/compiler.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, status)\u001b[0m\n\u001b[1;32m    255\u001b[0m                     \u001b[0;31m# No more fallback pipelines?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mis_final_pipeline\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0mpatched_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m                     \u001b[0;31m# Go to next fallback pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Caused By:\nTraceback (most recent call last):\n  File \"/home/student/anaconda2/envs/mlp/lib/python2.7/site-packages/numba/compiler.py\", line 249, in run\n    stage()\n  File \"/home/student/anaconda2/envs/mlp/lib/python2.7/site-packages/numba/compiler.py\", line 459, in stage_nopython_frontend\n    legalize_given_types(self.args, self.return_type)\n  File \"/home/student/anaconda2/envs/mlp/lib/python2.7/site-packages/numba/compiler.py\", line 727, in legalize_given_types\n    \"mode\" % (i, a))\nTypeError: Arg 0 of pyobject is not legal in nopython mode\n\nFailed at nopython (nopython frontend)\nArg 0 of pyobject is not legal in nopython mode"
     ]
    }
   ],
   "source": [
    "num_epochs = 30\n",
    "\n",
    "input_dim, output_dim, hidden_dim = 784, 10, 100\n",
    "\n",
    "model = MultipleLayerModel([\n",
    "    AffineLayer(input_dim, hidden_dim, weights_init, biases_init),\n",
    "    SigmoidLayer(),\n",
    "    AffineLayer(hidden_dim, hidden_dim, weights_init, biases_init), \n",
    "    SigmoidLayer(),\n",
    "    AffineLayer(hidden_dim, output_dim, weights_init, biases_init)\n",
    "])\n",
    "\n",
    "learning_rate = 1.1\n",
    "learning_rule = GradientDescentLearningRule(learning_rate=learning_rate)\n",
    "\n",
    "#stats, keys, fig_1, ax_1, fig_2, ax_2 = train_model_and_plot_stats(\n",
    "stats, keys, fig_1, ax_1, fig_2, ax_2, runTime = train_model_and_plot_stats(\n",
    "    model, error, \n",
    "    learning_rule,\n",
    "    train_data, valid_data, num_epochs, stats_interval,\n",
    "    schedulers = [ConstantLearningRateScheduler(learning_rate)]\n",
    ")\n",
    "\n",
    "print \"runtime: \" + str(runTime)\n",
    "\n",
    "getFinalValues(stats)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:mlp]",
   "language": "python",
   "name": "conda-env-mlp-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
